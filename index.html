<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Wenxin Fu - Homepage</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <!-- Personal Info Card -->
    <section class="card profile-card">
        <div class="card-left">
            <h1><strong>Wenxin Fu</strong></h1>
            <p class="position">Master Student</p>
            <p class="affiliation">Beijing University of Posts and Telecommunications</p>
            <p class="interests">Audio-driven Human Animation, Conversation, Diffusion Models, Multimodal Learning</p>
            <div class="social-links">
                <a href="https://github.com/Wx-Fu" target="_blank">GitHub</a>
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=72igOSwAAAAJ" target="_blank">Google Scholar</a>
            </div>
        </div>
        <div class="card-right">
            <!-- 修改：用 a 标签包裹 img，并添加 class -->
            <a href="images/avatar.jpg" class="lightbox-trigger">
                <img src="images/avatar.jpg" alt="Profile Photo">
            </a>
        </div>
    </section>

    <!-- Publications -->
    <h2 class="section-title">Publications</h2>

    <section class="publication-list">
        <!-- Paper 1 -->
        <div class="card publication-card">
            <div class="card-left">
                <!-- 修改：用 a 标签包裹 img，并添加 class -->
                <a href="images/paper3.jpg" class="lightbox-trigger">
                    <img src="images/paper3.jpg" alt="Paper Image">
                </a>
            </div>
            <div class="card-right">
                <span class="badge">MM 2025</span>
                <h3 class="paper-title">EMO-Avatar: An LLM-Agent-Orchestrated Framework for Multimodal Emotional Support in Human Animation</h3>
                <p class="paper-authors">
                    Keqi Chen, <strong>Wenxin Fu</strong>, Qihang Lu, Zekai Sun, Yizhong Geng, Yi Liu, Puyuan Guo, Yingming Gao, Ya Li
                </p>
                <p class="paper-summary">
                    To address the empathy gap in chatbots, we propose EMO-Avatar, an LLM-agent framework that generates expressive human animations for emotional support. It uniquely integrates Hill’s three-stage counseling theory to guide its reasoning, enabling adaptive generation of speech, expressions, and body language for deeper therapeutic interactions. EMO-Avatar achieved a top-2 rank in the AvaMERG Challenge, demonstrating superior performance in appropriateness, consistency, naturalness, and emotional expressiveness.
                </p>
                <a class="btn" href="https://dl.acm.org/doi/10.1145/3746027.3762030" target="_blank">Read Paper</a>
            </div>
        </div>
        
        <div class="card publication-card">
            <div class="card-left">
                <!-- 修改：用 a 标签包裹 img，并添加 class -->
                <a href="images/paper1.jpg" class="lightbox-trigger">
                    <img src="images/paper1.jpg" alt="Paper Image">
                </a>
            </div>
            <div class="card-right">
                <span class="badge">AAAI 2025</span>
                <h3 class="paper-title">Controllable 3D Dance Generation using Diffusion-Based Transformer U-Net</h3>
                <p class="paper-authors">
                    Puyuan Guo, Tuo Hao, <strong>Wenxin Fu</strong>, Yingming Gao, Ya Li
                </p>
                <p class="paper-summary">
                    We introduce a controllable dance generation method based on diffusion models that uses 2D keypoint sequences to guide 3D motion synthesis. Experiments on AIST++ and in-the-wild videos show our approach is effective and offers improved controllability over existing methods.
                </p>
                <a class="btn" href="https://ojs.aaai.org/index.php/AAAI/article/view/32339" target="_blank">Read Paper</a>
            </div>
        </div>

        <!-- Paper 2 -->
        <div class="card publication-card">
            <div class="card-left">
                <!-- 修改：用 a 标签包裹 img，并添加 class -->
                <a href="images/paper2.jpg" class="lightbox-trigger">
                    <img src="images/paper2.jpg" alt="Paper Image">
                </a>
            </div>
            <div class="card-right">
                <span class="badge">InterSpeech 2025</span>
                <h3 class="paper-title">EEG-based Voice Conversion: Hearing the Voice of Your Brain</h3>
                <p class="paper-authors">
                    Yizhong Geng, <strong>Wenxin Fu</strong>, Qihang Lu, Bingsong Bai, Cong Wang, Yingming Gao, Ya Li
                </p>
                <p class="paper-summary">
                    We propose the first EEG-based zero-shot voice conversion system that maps EEG-derived features to target speaker characteristics via an alignment module and three-stage training with a speech-only pre-trained VC model. Experiments show reliable conversion without target data.
                </p>
                <a class="btn" href="https://www.isca-archive.org/interspeech_2025/geng25b_interspeech.pdf" target="_blank">Read Paper</a>
            </div>
        </div>
    </section>

    <!-- 这是新增的图片放大功能所需的 HTML 结构，由 JS 动态创建 -->
    <!-- 你不需要手动添加这个，但知道它的存在有助于理解 -->
    <!-- 
    <div id="lightbox" class="lightbox">
        <span class="lightbox-close">&times;</span>
        <img class="lightbox-content" id="lightbox-img">
    </div> 
    -->

    <!-- 新增：引入 JavaScript 代码 -->
    <script src="main.js"></script>

</body>
</html>
