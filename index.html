<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Wenxin Fu - Homepage</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <!-- Personal Card -->
    <section class="profile-card">
        <div class="profile-info">
            <h1><strong>Wenxin Fu</strong></h1>
            <p class="position">Master Student</p>
            <p class="affiliation">Beijing University of Posts and Telecommunications</p>
            <p class="interests">Audio-driven Human Animation | Diffusion Models | Multimodal Learning</p>
        </div>
        <div class="profile-photo">
            <img src="images/avatar.jpg" alt="Profile Photo">
        </div>
    </section>

    <!-- Publications -->
    <section class="publications">
        <h2>Publications</h2>
        <div class="cards">

            <!-- Paper Card -->
            <div class="card">
                <div class="card-header">
                    <span class="badge">AAAI 2025</span>
                    <h3 class="paper-title">Controllable 3D Dance Generation using Diffusion-Based Transformer U-Net</h3>
                </div>
                <p class="paper-authors">
                    Puyuan Guo, Tuo Hao, <strong>Wenxin Fu</strong>, Yingming Gao, Ya Li
                </p>
                <p class="paper-summary">
                    We introduce a controllable dance generation method based on diffusion models that uses 2D keypoint sequences to guide 3D motion synthesis. Experiments on AIST++ and in-the-wild videos show our approach is effective and offers improved controllability.
                </p>
                <a class="btn" href="https://ojs.aaai.org/index.php/AAAI/article/view/32339" target="_blank">Read Paper</a>
            </div>

            <!-- Paper Card -->
            <div class="card">
                <div class="card-header">
                    <span class="badge">InterSpeech 2025</span>
                    <h3 class="paper-title">EEG-based Voice Conversion: Hearing the Voice of Your Brain</h3>
                </div>
                <p class="paper-authors">
                    Yizhong Geng, <strong>Wenxin Fu</strong>, Qihang Lu, Bingsong Bai, Cong Wang, Yingming Gao, Ya Li
                </p>
                <p class="paper-summary">
                    We propose the first EEG-based zero-shot voice conversion system that maps EEG-derived features to target speaker characteristics via an alignment module and three-stage training with a speech-only pre-trained VC model. Experiments show reliable conversion without target data.
                </p>
                <a class="btn" href="https://www.isca-archive.org/interspeech_2025/geng25b_interspeech.pdf" target="_blank">Read Paper</a>
            </div>

        </div>
    </section>

</body>
</html>
